{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Text Factorizing with NLP\n",
    "## Thomas W Ludlow Jr\n",
    "### General Assembly DSI-NY-6, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will use Natural Language Processing (NLP) tools such as Python's NLTK to \"factorize\" a passage of text by identifying and quantifying similarities between the passage and historical texts and literature.  Starting with sentences then expanding to paragraphs and longer, I will identify grammatical patterns and word similarities and measure the amount of stylistic and content alignment with historical/philosophical text and classic literature, available online at the Gutenberg Project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tools_\n",
    " - Sci-Kit Learn\n",
    " - Natural Language Toolkit (NLTK)\n",
    " - Keras\n",
    " - TensorFlow\n",
    " - Gensim\n",
    " - spaCy\n",
    "\n",
    "_Preprocessing_\n",
    " - Tokenizing\n",
    " - Stemming/Lemmatizing\n",
    " - n-grams of 2+\n",
    " - Part of Speech Tagging\n",
    " - Dependency Modeling\n",
    " - Topic Modeling\n",
    "\n",
    "_Models_\n",
    " - Literal similarity\n",
    "  - Lemmatize and compare n-grams to corpora with string functionality\n",
    "  - Token Vectors for word counts\n",
    "  - Part of Speech Tagging to identify most-common word orders of different lengths\n",
    "  - Dependency modeling to group connected words\n",
    " - Sentiment Analysis\n",
    " - Word2Vec\n",
    "  - Cosine similarity to identify similarity between passage and corpora\n",
    " - Keras Neural Net\n",
    "  - Multi-classification Model of corpus inputs +1\n",
    "  - Sigmoid results showing levels of similarity with one to represent dissimilarity\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - spaCy, sklearn for Keras RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains exploration of spaCy libraries and methods.  Once functionality of various tools is established, an approach for optimizing against large bodies of text on hosted processors (AWS) can be developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Philosophy Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plato_file = open('./data/plato_republic.txt','r')\n",
    "aristotle_file = open('./data/aristotle_categories.txt','r')\n",
    "descartes_file = open('./data/descartes_principles.txt','r')\n",
    "kant_file = open('./data/kant_critique.txt','r')\n",
    "\n",
    "plato = plato_file.readlines()\n",
    "aristotle = aristotle_file.readlines()\n",
    "descartes = descartes_file.readlines()\n",
    "kant = kant_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plato_lines = plato[8494:24328] # contents of work\n",
    "aristotle_lines = aristotle[37:1492] # contents of work\n",
    "descartes_lines = descartes[362:-6] # contents of work\n",
    "kant_lines = kant[27:-373] # contents of work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paragraph Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4552\n",
      "14029\n"
     ]
    }
   ],
   "source": [
    "for n, line in enumerate(plato_lines):\n",
    "    if \"knowledge\" in line.lower() and \"virtue\" in line.lower(): \n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Those then who know not wisdom and virtue, and are always busy with\\n',\n",
       " 'gluttony and sensuality, go down and up again as far as the mean; and\\n',\n",
       " 'in this region they move at random throughout life, but they never pass\\n',\n",
       " 'into the true upper world; thither they neither look, nor do they ever\\n',\n",
       " 'find their way, neither are they truly filled with true being, nor do\\n',\n",
       " 'they taste of pure and abiding pleasure. Like cattle, with their eyes\\n',\n",
       " 'always looking down and their heads stooping to the earth, that is,\\n',\n",
       " 'to the dining-table, they fatten and feed and breed, and, in their\\n',\n",
       " 'excessive love of these delights, they kick and butt at one another with\\n',\n",
       " 'horns and hoofs which are made of iron; and they kill one another by\\n',\n",
       " 'reason of their insatiable lust. For they fill themselves with that\\n',\n",
       " 'which is not substantial, and the part of themselves which they fill is\\n',\n",
       " 'also unsubstantial and incontinent.\\n']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plato_lines[14077:14090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plato_par = plato_lines[14077:14090]\n",
    "aristotle_par = aristotle_lines[983:995]\n",
    "descartes_par = descartes_lines[3052:3077]\n",
    "kant_par = kant_lines[20381:20400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_str = ' '.join(aristotle_par).replace('\\n','') # convert list of strings sep by '\\n' into single string\n",
    "pl_str = ' '.join(plato_par).replace('\\n','')\n",
    "de_str = ' '.join(descartes_par).replace('\\n','')\n",
    "ka_str = ' '.join(kant_par).replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_str = ' '.join(aristotle_lines).replace('\\n','') # convert list of strings sep by '\\n' into single string\n",
    "pl_str = ' '.join(plato_lines).replace('\\n','')\n",
    "de_str = ' '.join(descartes_lines).replace('\\n','')\n",
    "ka_str = ' '.join(kant_lines).replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1_500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = nlp(ar_str)\n",
    "doc_p = nlp(pl_str)\n",
    "doc_d = nlp(de_str)\n",
    "doc_k = nlp(ka_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aristotle Aristotle : 1.0\n",
      "Aristotle Plato : 0.986406569347\n",
      "Aristotle Descartes : 0.99242321186\n",
      "Aristotle Kant : 0.990772543316\n",
      "\n",
      "Plato Aristotle : 0.986406569347\n",
      "Plato Plato : 1.0\n",
      "Plato Descartes : 0.993415082399\n",
      "Plato Kant : 0.980461739495\n",
      "\n",
      "Descartes Aristotle : 0.99242321186\n",
      "Descartes Plato : 0.993415082399\n",
      "Descartes Descartes : 1.0\n",
      "Descartes Kant : 0.993687672486\n",
      "\n",
      "Kant Aristotle : 0.990772543316\n",
      "Kant Plato : 0.980461739495\n",
      "Kant Descartes : 0.993687672486\n",
      "Kant Kant : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [doc_a, doc_p, doc_d, doc_k]\n",
    "doc_names = ['Aristotle', 'Plato', 'Descartes', 'Kant']\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    for j in range(len(docs)):\n",
    "        print(doc_names[i], doc_names[j], \":\", docs[i].similarity(docs[j]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We -PRON- PRON PRP nsubjpass Xx True False\n",
      "must must VERB MD aux xxxx True False\n",
      "not not ADV RB neg xxx True False\n",
      "be be VERB VB auxpass xx True False\n",
      "disturbed disturb VERB VBN ROOT xxxx True False\n",
      "because because ADP IN mark xxxx True False\n",
      "it -PRON- PRON PRP nsubjpass xx True False\n",
      "may may VERB MD aux xxx True False\n",
      "be be VERB VB auxpass xx True False\n",
      "argued argue VERB VBN advcl xxxx True False\n",
      "that that ADP IN mark xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "though though ADP IN mark xxxx True False\n",
      "proposing propose VERB VBG advcl xxxx True False\n",
      "to to PART TO aux xx True False\n",
      "discuss discuss VERB VB xcomp xxxx True False\n",
      "the the DET DT det xxx True False\n",
      "category category NOUN NN dobj xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "quality quality NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "we -PRON- PRON PRP nsubj xx True False\n",
      "have have VERB VBP aux xxxx True False\n",
      "included include VERB VBN ccomp xxxx True False\n",
      "in in ADP IN prep xx True False\n",
      "it -PRON- PRON PRP pobj xx True False\n",
      "many many ADJ JJ amod xxxx True False\n",
      "relative relative ADJ JJ amod xxxx True False\n",
      "terms term NOUN NNS dobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "We -PRON- PRON PRP nsubj Xx True False\n",
      "did do VERB VBD aux xxx True False\n",
      "say say VERB VB ROOT xxx True False\n",
      "that that ADP IN mark xxxx True False\n",
      "habits habit NOUN NNS nsubj xxxx True False\n",
      "and and CCONJ CC cc xxx True False\n",
      "dispositions disposition NOUN NNS conj xxxx True False\n",
      "were be VERB VBD ccomp xxxx True False\n",
      "relative relative ADJ JJ acomp xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "In in ADP IN prep Xx True False\n",
      "practically practically ADV RB advmod xxxx True False\n",
      "all all DET DT nummod xxx True False\n",
      "such such ADJ JJ amod xxxx True False\n",
      "cases case NOUN NNS pobj xxxx True False\n",
      "the the DET DT det xxx True False\n",
      "genus genus NOUN NN nsubj xxxx True False\n",
      "is be VERB VBZ ROOT xx True False\n",
      "relative relative ADJ JJ acomp xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "the the DET DT det xxx True False\n",
      "individual individual NOUN NN appos xxxx True False\n",
      "not not ADV RB neg xxx True False\n",
      ". . PUNCT . punct . False False\n",
      "Thus thus ADV RB advmod Xxxx True False\n",
      "knowledge knowledge NOUN NN nsubjpass xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "as as ADP IN prep xx True False\n",
      "a a DET DT det x True False\n",
      "genus genus NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "is be VERB VBZ auxpass xx True False\n",
      "explained explain VERB VBN ROOT xxxx True False\n",
      "by by ADP IN agent xx True False\n",
      "reference reference NOUN NN pobj xxxx True False\n",
      "to to ADP IN prep xx True False\n",
      "something something NOUN NN pobj xxxx True False\n",
      "else else ADV RB advmod xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "for for ADP IN mark xxx True False\n",
      "we -PRON- PRON PRP nsubj xx True False\n",
      "mean mean VERB VBP advcl xxxx True False\n",
      "a a DET DT det x True False\n",
      "knowledge knowledge NOUN NN dobj xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "something something NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "But but CCONJ CC cc Xxx True False\n",
      "particular particular ADJ JJ amod xxxx True False\n",
      "branches branch NOUN NNS nsubjpass xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "knowledge knowledge NOUN NN pobj xxxx True False\n",
      "are be VERB VBP auxpass xxx True False\n",
      "not not ADV RB neg xxx True False\n",
      "thus thus ADV RB advmod xxxx True False\n",
      "explained explain VERB VBN ROOT xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "The the DET DT det Xxx True False\n",
      "knowledge knowledge NOUN NN nsubj xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "grammar grammar NOUN NN pobj xxxx True False\n",
      "is be VERB VBZ ccomp xx True False\n",
      "not not ADV RB neg xxx True False\n",
      "relative relative ADJ JJ acomp xxxx True False\n",
      "to to ADP IN prep xx True False\n",
      "anything anything NOUN NN pobj xxxx True False\n",
      "external external ADJ JJ amod xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "nor nor CCONJ CC cc xxx True False\n",
      "is be VERB VBZ conj xx True False\n",
      "the the DET DT det xxx True False\n",
      "knowledge knowledge NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "music music NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "but but CCONJ CC cc xxx True False\n",
      "these these DET DT nsubj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "if if ADP IN mark xx True False\n",
      "relative relative ADJ JJ advcl xxxx True False\n",
      "at at ADV RB advmod xx True False\n",
      "all all ADV RB advmod xxx True False\n",
      ", , PUNCT , punct , False False\n",
      "are be VERB VBP conj xxx True False\n",
      "relative relative ADJ JJ acomp xxxx True False\n",
      "only only ADV RB advmod xxxx True False\n",
      "in in ADP IN prep xx True False\n",
      "virtue virtue NOUN NN pobj xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "their -PRON- ADJ PRP$ poss xxxx True False\n",
      "genera genus NOUN NNS pobj xxxx True False\n",
      "; ; PUNCT : punct ; False False\n",
      "thus thus ADV RB advmod xxxx True False\n",
      "grammar grammar NOUN NN nsubjpass xxxx True False\n",
      "is be VERB VBZ auxpass xx True False\n",
      "said say VERB VBN ccomp xxxx True False\n",
      "be be VERB VB xcomp xx True False\n",
      "the the DET DT det xxx True False\n",
      "knowledge knowledge NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "something something NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "not not ADV RB neg xxx True False\n",
      "the the DET DT det xxx True False\n",
      "grammar grammar NOUN NN appos xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "something something NOUN NN pobj xxxx True False\n",
      "; ; PUNCT : punct ; False False\n",
      "similarly similarly ADV RB advmod xxxx True False\n",
      "music music NOUN NN nsubj xxxx True False\n",
      "is be VERB VBZ ROOT xx True False\n",
      "the the DET DT det xxx True False\n",
      "knowledge knowledge NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "something something NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "not not ADV RB neg xxx True False\n",
      "the the DET DT det xxx True False\n",
      "music music NOUN NN appos xxxx True False\n",
      "of of ADP IN prep xx True False\n",
      "something something NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc_a:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy Noun Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I nsubj wrong\n",
      "the truth truth dobj wrong\n",
      "it it nsubj certain\n",
      "it it nsubj is\n",
      "I I nsubj distinguish\n",
      "two kinds kinds dobj distinguish\n",
      "certitude certitude pobj of\n",
      "a certainty certainty nsubj sufficient\n",
      "the conduct conduct pobj for\n",
      "life life pobj of\n",
      "we we nsubj look\n",
      "the absolute power power pobj to\n",
      "God God pobj of\n",
      "what what nsubj is\n",
      "who who nsubj visited\n",
      "Rome Rome dobj visited\n",
      "it it nsubj is\n",
      "a city city attr is\n",
      "Italy Italy pobj of\n",
      "it it nsubj be\n",
      "whom whom pobj from\n",
      "they they nsubj got\n",
      "their information information nsubjpass deceived\n",
      "any one one nsubj bethinks\n",
      "a letter letter dobj decipher\n",
      "Latin characters characters pobj in\n",
      "regular order order pobj in\n",
      "himself himself dobj bethinks\n",
      "a B B dobj reading\n",
      "an A A nsubjpass found\n",
      "a B B attr is\n",
      "place place pobj in\n",
      "each letter letter pobj of\n",
      "the one one dobj substituting\n",
      "it it dobj follows\n",
      "the order order pobj in\n",
      "the alphabet alphabet pobj of\n",
      "he he nsubj finds\n",
      "certain Latin words words attr are\n",
      "he he nsubj doubt\n",
      "the true meaning meaning nsubjpass contained\n",
      "the writing writing pobj of\n",
      "these words words pobj in\n",
      "he he nsubj discover\n",
      "conjecture conjecture pobj by\n",
      "it it nsubj is\n",
      "the writer writer nsubj arrange\n",
      "it it pobj of\n",
      "the letters letters dobj arrange\n",
      "this principle principle pobj on\n",
      "alphabetical order order pobj of\n",
      "another meaning meaning dobj concealed\n",
      "it it pobj in\n",
      "the cipher cipher nsubj contains\n",
      "a number number dobj contains\n",
      "words words pobj of\n",
      "they they nsubjpass deduced\n",
      "who who nsubj observe\n",
      "how many things things dobj observe\n",
      "the magnet magnet pobj regarding\n",
      "fire fire conj magnet\n",
      "the fabric fabric conj fire\n",
      "the whole world world pobj of\n",
      "a very small number number pobj from\n",
      "principles principles pobj of\n",
      "they they nsubj deemed\n",
      "I I nsubj taken\n",
      "them them dobj taken\n",
      "grounds grounds pobj without\n",
      "it it nsubj happen\n",
      "so many things things nsubj cohere\n",
      "these principles principles nsubj were\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc_d.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two 114 117 CARDINAL\n",
      "first 142 147 ORDINAL\n",
      "Rome 340 344 GPE\n",
      "Italy 379 384 GPE\n",
      "Latin 526 531 LANGUAGE\n",
      "Latin 825 830 LANGUAGE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc_d.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rational cognition is, again, based either on conceptions, or on the construction of conceptions.\n",
      "The former is termed philosophical, the latter mathematical.\n",
      "I have already shown the essential difference of these two methods of cognition in the first chapter.\n",
      "A cognition may be objectively philosophical and subjectively historical—as is the case with the majority of scholars and those who cannot look beyond the limits of their system, and who remain in a state of pupilage all their lives.\n",
      "But it is remarkable that mathematical knowledge, when committed to memory, is valid, from the subjective point of view, as rational knowledge also, and that the same distinction cannot be drawn here as in the case of philosophical cognition.\n",
      "The reason is that the only way of arriving at this knowledge is through the essential principles of reason, and thus it is always certain and indisputable; because reason is employed in concreto—but at the same time a priori—that is, in pure and, therefore, infallible intuition; and thus all causes of illusion and error are excluded.\n",
      "Of all the a priori sciences of reason, therefore, mathematics alone can be learned.\n",
      "Philosophy—unless it be in an historical manner—cannot be learned; we can at most learn to philosophize.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc_k.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit TFIDF on all docs combined\n",
    "2. Create DF for each corpus transformed\n",
    "3. Combine DF and add classifier value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc_a, doc_p, doc_d, doc_k]\n",
    "doc_names = ['Aristotle', 'Plato', 'Descartes', 'Kant', 'No Author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter and combine lemma tokens for TFIDF fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_tokens_all = []\n",
    "lem_tokens_ind = []\n",
    "for doc in docs:\n",
    "    lem_tokens_all.extend([token.lemma_ for token in doc \n",
    "                           if (token.lemma_ != '-PRON-') \n",
    "                           and (token.pos_ != 'PUNCT') \n",
    "                           and (len(token.lemma_)) > 1])\n",
    "    lem_tokens_ind.append(' '.join([token.lemma_ for token in doc \n",
    "                           if (token.lemma_ != '-PRON-') \n",
    "                           and (token.pos_ != 'PUNCT') \n",
    "                           and (len(token.lemma_)) > 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342217\n",
      "4\n",
      "74093\n",
      "553000\n",
      "156768\n",
      "1152929\n"
     ]
    }
   ],
   "source": [
    "print(len(lem_tokens_all))\n",
    "print(len(lem_tokens_ind))\n",
    "for i in range(4):\n",
    "    print(len(lem_tokens_ind[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'must not be disturb because may be argue that though propose to discuss the category of quality have include in many relative term do say that habit and disposition be relative in practically all such case the genus be relative the individual not thus knowledge as genus be explain by reference to something else for mean knowledge of something but particular branch of knowledge be not thus explain the knowledge of grammar be not relative to anything external nor be the knowledge of music but these if relative at all be relative only in virtue of genus thus grammar be say be the knowledge of something not the grammar of something similarly music be the knowledge of something not the music of something'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tokens_ind[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_df = pd.DataFrame(tfidf.fit_transform(lem_tokens_ind).toarray(), columns=tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>1596</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>youngster</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeno</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 8198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000        10       100        11        12        13        14  \\\n",
       "0  0.000000  0.000628  0.000000  0.000775  0.000775  0.000775  0.000775   \n",
       "1  0.000108  0.000087  0.001234  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000056  0.000182  0.000000  0.000112  0.000337  0.000169  0.000112   \n",
       "\n",
       "         15      1596        16    ...     youngster     yours  yourself  \\\n",
       "0  0.000775  0.000000  0.000000    ...      0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000    ...      0.000137  0.000137  0.000432   \n",
       "2  0.000000  0.000522  0.000000    ...      0.000000  0.000000  0.000000   \n",
       "3  0.000169  0.000000  0.000285    ...      0.000000  0.000000  0.000225   \n",
       "\n",
       "      youth  youthful      zeal   zealous      zeno      zero      zeus  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.005249  0.000216  0.000175  0.000000  0.000000  0.000000  0.002193  \n",
       "2  0.000999  0.000000  0.000666  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000182  0.000056  0.000046  0.000071  0.000071  0.000285  0.000000  \n",
       "\n",
       "[4 rows x 8198 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_df.loc[corp_df.shape[0], :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_df['AUTHOR'] = doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>1596</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeno</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "      <th>AUTHOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Aristotle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>Plato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Descartes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Author</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000        10       100        11        12        13        14  \\\n",
       "0  0.000000  0.000628  0.000000  0.000775  0.000775  0.000775  0.000775   \n",
       "1  0.000108  0.000087  0.001234  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000056  0.000182  0.000000  0.000112  0.000337  0.000169  0.000112   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         15      1596        16    ...         yours  yourself     youth  \\\n",
       "0  0.000775  0.000000  0.000000    ...      0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000    ...      0.000137  0.000432  0.005249   \n",
       "2  0.000000  0.000522  0.000000    ...      0.000000  0.000000  0.000999   \n",
       "3  0.000169  0.000000  0.000285    ...      0.000000  0.000225  0.000182   \n",
       "4  0.000000  0.000000  0.000000    ...      0.000000  0.000000  0.000000   \n",
       "\n",
       "   youthful      zeal   zealous      zeno      zero      zeus     AUTHOR  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  Aristotle  \n",
       "1  0.000216  0.000175  0.000000  0.000000  0.000000  0.002193      Plato  \n",
       "2  0.000000  0.000666  0.000000  0.000000  0.000000  0.000000  Descartes  \n",
       "3  0.000056  0.000046  0.000071  0.000071  0.000285  0.000000       Kant  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  No Author  \n",
       "\n",
       "[5 rows x 8199 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X and y definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corp_df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>1596</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>youngster</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeno</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000        10       100        11        12        13        14  \\\n",
       "0  0.000000  0.000628  0.000000  0.000775  0.000775  0.000775  0.000775   \n",
       "1  0.000108  0.000087  0.001234  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000056  0.000182  0.000000  0.000112  0.000337  0.000169  0.000112   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         15      1596        16    ...     youngster     yours  yourself  \\\n",
       "0  0.000775  0.000000  0.000000    ...      0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000    ...      0.000137  0.000137  0.000432   \n",
       "2  0.000000  0.000522  0.000000    ...      0.000000  0.000000  0.000000   \n",
       "3  0.000169  0.000000  0.000285    ...      0.000000  0.000000  0.000225   \n",
       "4  0.000000  0.000000  0.000000    ...      0.000000  0.000000  0.000000   \n",
       "\n",
       "      youth  youthful      zeal   zealous      zeno      zero      zeus  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.005249  0.000216  0.000175  0.000000  0.000000  0.000000  0.002193  \n",
       "2  0.000999  0.000000  0.000666  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000182  0.000056  0.000046  0.000071  0.000071  0.000285  0.000000  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 8198 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8198)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = corp_df.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = pd.get_dummies(y).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X.shape[0], input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(5, activation=None))\n",
    "model.add(Activation(tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.6110 - acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 523us/step - loss: 1.6101 - acc: 0.2000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 426us/step - loss: 1.6096 - acc: 0.2000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 469us/step - loss: 1.6082 - acc: 0.6000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 499us/step - loss: 1.6083 - acc: 0.6000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 507us/step - loss: 1.6086 - acc: 0.4000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 480us/step - loss: 1.6078 - acc: 0.6000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 478us/step - loss: 1.6065 - acc: 0.8000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 411us/step - loss: 1.6059 - acc: 0.6000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 514us/step - loss: 1.6047 - acc: 0.6000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 486us/step - loss: 1.6032 - acc: 0.4000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 491us/step - loss: 1.6056 - acc: 0.2000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 480us/step - loss: 1.6002 - acc: 0.6000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 494us/step - loss: 1.5992 - acc: 0.4000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 509us/step - loss: 1.6003 - acc: 0.6000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 428us/step - loss: 1.5943 - acc: 0.6000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 464us/step - loss: 1.5990 - acc: 0.6000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 435us/step - loss: 1.5980 - acc: 0.6000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 525us/step - loss: 1.5980 - acc: 0.4000\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 574us/step - loss: 1.5893 - acc: 0.6000\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 524us/step - loss: 1.5968 - acc: 0.6000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 506us/step - loss: 1.5888 - acc: 0.6000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 469us/step - loss: 1.5933 - acc: 0.6000\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 581us/step - loss: 1.5968 - acc: 0.6000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 517us/step - loss: 1.5915 - acc: 0.4000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 444us/step - loss: 1.5869 - acc: 0.6000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 395us/step - loss: 1.5752 - acc: 0.6000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 376us/step - loss: 1.5742 - acc: 0.6000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 396us/step - loss: 1.5713 - acc: 0.6000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 429us/step - loss: 1.5692 - acc: 0.6000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 485us/step - loss: 1.5836 - acc: 0.6000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 428us/step - loss: 1.5822 - acc: 0.6000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 470us/step - loss: 1.5631 - acc: 0.6000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 517us/step - loss: 1.5782 - acc: 0.6000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 476us/step - loss: 1.5564 - acc: 0.6000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 393us/step - loss: 1.6033 - acc: 0.4000\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 422us/step - loss: 1.5508 - acc: 0.6000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 408us/step - loss: 1.5491 - acc: 0.6000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 370us/step - loss: 1.5993 - acc: 0.4000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 428us/step - loss: 1.5634 - acc: 0.4000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 544us/step - loss: 1.5545 - acc: 0.4000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 425us/step - loss: 1.5654 - acc: 0.6000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 497us/step - loss: 1.5356 - acc: 0.6000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 364us/step - loss: 1.5330 - acc: 0.6000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 402us/step - loss: 1.5298 - acc: 0.6000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 396us/step - loss: 1.5538 - acc: 0.6000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 419us/step - loss: 1.5487 - acc: 0.6000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5717 - acc: 0.4000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 364us/step - loss: 1.5184 - acc: 0.6000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 391us/step - loss: 1.5112 - acc: 0.6000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 431us/step - loss: 1.5079 - acc: 0.6000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 343us/step - loss: 1.5046 - acc: 0.6000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 441us/step - loss: 1.4992 - acc: 0.6000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 464us/step - loss: 1.5468 - acc: 0.6000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 471us/step - loss: 1.5449 - acc: 0.6000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5584 - acc: 0.6000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 343us/step - loss: 1.5166 - acc: 0.6000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 342us/step - loss: 1.5223 - acc: 0.4000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 373us/step - loss: 1.5030 - acc: 0.6000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 561us/step - loss: 1.4728 - acc: 0.6000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 411us/step - loss: 1.5038 - acc: 0.6000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 410us/step - loss: 1.4651 - acc: 0.6000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 364us/step - loss: 1.5166 - acc: 0.6000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 417us/step - loss: 1.4518 - acc: 0.6000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 409us/step - loss: 1.4515 - acc: 0.6000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 429us/step - loss: 1.4794 - acc: 0.6000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 355us/step - loss: 1.4389 - acc: 0.6000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 484us/step - loss: 1.5396 - acc: 0.6000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 510us/step - loss: 1.4762 - acc: 0.6000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 447us/step - loss: 1.4326 - acc: 0.6000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 368us/step - loss: 1.4676 - acc: 0.6000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 358us/step - loss: 1.4217 - acc: 0.6000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 353us/step - loss: 1.4118 - acc: 0.6000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 348us/step - loss: 1.5300 - acc: 0.6000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 387us/step - loss: 1.3989 - acc: 0.8000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 387us/step - loss: 1.4513 - acc: 0.6000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 458us/step - loss: 1.3908 - acc: 0.6000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 374us/step - loss: 1.4304 - acc: 0.6000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 441us/step - loss: 1.4521 - acc: 0.4000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 340us/step - loss: 1.4564 - acc: 0.6000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 334us/step - loss: 1.4210 - acc: 0.6000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 371us/step - loss: 1.4687 - acc: 0.6000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 418us/step - loss: 1.4170 - acc: 0.6000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 353us/step - loss: 1.4312 - acc: 0.6000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 459us/step - loss: 1.4569 - acc: 0.6000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 481us/step - loss: 1.3958 - acc: 0.6000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 524us/step - loss: 1.3922 - acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 447us/step - loss: 1.3833 - acc: 0.6000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 405us/step - loss: 1.3237 - acc: 0.6000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 363us/step - loss: 1.3175 - acc: 0.6000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 394us/step - loss: 1.4595 - acc: 0.4000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 454us/step - loss: 1.3132 - acc: 0.6000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 372us/step - loss: 1.3667 - acc: 0.6000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 381us/step - loss: 1.2942 - acc: 0.6000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 384us/step - loss: 1.4109 - acc: 0.4000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 359us/step - loss: 1.2810 - acc: 0.8000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 446us/step - loss: 1.2938 - acc: 0.6000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 450us/step - loss: 1.2644 - acc: 0.8000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 585us/step - loss: 1.4942 - acc: 0.6000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 401us/step - loss: 1.2548 - acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a93aec588>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y_cat, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23568825,  0.17034306,  0.22159944,  0.19413988,  0.17822932],\n",
       "       [ 0.16509713,  0.17328736,  0.16697466,  0.20128256,  0.2933583 ],\n",
       "       [ 0.1489836 ,  0.25180736,  0.24150878,  0.16304421,  0.19465606],\n",
       "       [ 0.1617444 ,  0.17601646,  0.41114497,  0.13117988,  0.11991429],\n",
       "       [ 0.22483061,  0.18815619,  0.1821624 ,  0.2051661 ,  0.19968472]], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_ds_nn",
   "language": "python",
   "name": "python3_ds_nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
